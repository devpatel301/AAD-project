{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de7df5e7",
   "metadata": {},
   "source": [
    "# Comprehensive Maximum Clique Algorithm Benchmarking\n",
    "\n",
    "This notebook automatically runs all 11 C++ maximum clique algorithms on all datasets and generates detailed performance analysis.\n",
    "\n",
    "**Algorithms tested:**\n",
    "1. **Greedy** - Fast heuristic approximation\n",
    "2. **Randomized** - Local search with random restarts\n",
    "3. **Simulated Annealing** - Metaheuristic optimization\n",
    "4. **Bron-Kerbosch** - Basic exact algorithm (skipped if >1000 vertices OR density >0.5)\n",
    "5. **Tomita** - BK with pivot optimization (exact)\n",
    "6. **Degeneracy BK** - BK with degeneracy ordering (exact)\n",
    "7. **√ñsterg√•rd** - Branch-and-bound with coloring (exact)\n",
    "8. **BBMC** - Bitset-based branch-and-bound (exact)\n",
    "9. **CPU Optimized** - Bitset-optimized (exact, ‚â§8192 vertices)\n",
    "10. **MaxCliqueDyn** - Tomita with dynamic coloring (exact)\n",
    "\n",
    "**Output columns:**\n",
    "- Dataset name, vertices, edges, density\n",
    "- Max degree, average degree, degeneracy\n",
    "- Clique size, wall-clock time, memory usage per algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8787aad4",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f51c5fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (16, 8)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb18f486",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "168997b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded\n",
      "   Benchmark source: benchmark_comprehensive.cpp\n",
      "   Results folder: benchmark_results\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BENCHMARK_SOURCE = \"benchmark_comprehensive.cpp\"\n",
    "BENCHMARK_EXECUTABLE = \"./benchmark_comprehensive\"\n",
    "DATASETS_FOLDERS = [\"datasets/real_world\", \"datasets/synthetic\", \"datasets/benchmark\"]\n",
    "RESULTS_FOLDER = \"benchmark_results\"\n",
    "\n",
    "# Create results folder\n",
    "os.makedirs(RESULTS_FOLDER, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"   Benchmark source: {BENCHMARK_SOURCE}\")\n",
    "print(f\"   Results folder: {RESULTS_FOLDER}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc674e6",
   "metadata": {},
   "source": [
    "## 3. Compile Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f235e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî® Compiling benchmark...\n",
      "‚úÖ Compilation successful!\n",
      "   Executable: ./benchmark_comprehensive\n",
      "‚úÖ Compilation successful!\n",
      "   Executable: ./benchmark_comprehensive\n"
     ]
    }
   ],
   "source": [
    "print(\"üî® Compiling benchmark...\")\n",
    "\n",
    "compile_cmd = [\n",
    "    \"g++\", \"-std=c++17\", \"-O3\",\n",
    "    BENCHMARK_SOURCE,\n",
    "    \"-o\", \"benchmark_all\"\n",
    "]\n",
    "\n",
    "result = subprocess.run(compile_cmd, capture_output=True, text=True)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(\"‚ùå Compilation failed:\")\n",
    "    print(result.stderr)\n",
    "    raise Exception(\"Compilation failed\")\n",
    "else:\n",
    "    print(\"‚úÖ Compilation successful!\")\n",
    "    print(f\"   Executable: {BENCHMARK_EXECUTABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a938b517",
   "metadata": {},
   "source": [
    "## 4. Discover Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c939edc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Found 22 datasets:\n",
      "\n",
      "\n",
      "BENCHMARK:\n",
      "  ‚Ä¢ C125.9.txt                               (    56.8 KB)\n",
      "  ‚Ä¢ C250.9.txt                               (   250.2 KB)\n",
      "  ‚Ä¢ brock400_2.txt                           (   552.8 KB)\n",
      "  ‚Ä¢ ca-GrQc.txt                              (   185.9 KB)\n",
      "  ‚Ä¢ email-Eu-core.txt                        (   156.0 KB)\n",
      "  ‚Ä¢ frb30-15-01.txt                          (   855.6 KB)\n",
      "  ‚Ä¢ frb35-17-01.txt                          (  1545.0 KB)\n",
      "  ‚Ä¢ gen200_p0.9_44.txt                       (   156.2 KB)\n",
      "  ‚Ä¢ keller4.txt                              (    80.6 KB)\n",
      "  ‚Ä¢ p_hat300-1.txt                           (   100.0 KB)\n",
      "  ‚Ä¢ queen5_5.txt                             (     0.0 KB)\n",
      "\n",
      "REAL_WORLD:\n",
      "  ‚Ä¢ facebook_combined.txt                    (  1006.9 KB)\n",
      "  ‚Ä¢ twitter_combined.txt                     ( 48234.1 KB)\n",
      "\n",
      "SYNTHETIC:\n",
      "  ‚Ä¢ random_180v.txt                          (    20.9 KB)\n",
      "  ‚Ä¢ rmat_er_large.txt                        (    74.4 KB)\n",
      "  ‚Ä¢ rmat_er_small.txt                        (    17.1 KB)\n",
      "  ‚Ä¢ rmat_sd1_large.txt                       (    91.3 KB)\n",
      "  ‚Ä¢ rmat_sd1_small.txt                       (    21.1 KB)\n",
      "  ‚Ä¢ rmat_sd2_large.txt                       (    89.2 KB)\n",
      "  ‚Ä¢ rmat_sd2_small.txt                       (    20.7 KB)\n",
      "  ‚Ä¢ sat3_large.txt                           (     4.1 KB)\n",
      "  ‚Ä¢ sat3_small.txt                           (     1.8 KB)\n"
     ]
    }
   ],
   "source": [
    "# Find all datasets\n",
    "all_datasets = []\n",
    "for folder in DATASETS_FOLDERS:\n",
    "    if os.path.exists(folder):\n",
    "        datasets = glob.glob(os.path.join(folder, \"*.txt\"))\n",
    "        all_datasets.extend([(folder, os.path.basename(d), d) for d in datasets])\n",
    "\n",
    "# Sort by category and name\n",
    "all_datasets.sort()\n",
    "\n",
    "print(f\"üìä Found {len(all_datasets)} datasets:\\n\")\n",
    "current_category = None\n",
    "for category, name, path in all_datasets:\n",
    "    if category != current_category:\n",
    "        current_category = category\n",
    "        category_name = category.split('/')[-1].upper()\n",
    "        print(f\"\\n{category_name}:\")\n",
    "    file_size = os.path.getsize(path) / 1024  # KB\n",
    "    print(f\"  ‚Ä¢ {name:40s} ({file_size:8.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e6a157",
   "metadata": {},
   "source": [
    "## 5. Run Benchmarks on All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09adba93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üöÄ Starting comprehensive benchmark suite\n",
      "   Time: 2025-11-20 07:08:45\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "[1/22] Running: C125.9.txt\n",
      "   Category: benchmark\n",
      "   Path: datasets/benchmark/C125.9.txt\n",
      "   üîÑ Starting benchmark execution...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìä Graph: 125 vertices, 6,963 edges, density=89.8452\n",
      "   ‚è±Ô∏è  Total benchmark time: 127.85s\n",
      "   \n",
      "   Algorithm Results:\n",
      "      ‚úì  Greedy                         ‚Üí Clique size:  29, Time:   0.0000s\n",
      "      ‚úì  Randomized                     ‚Üí Clique size:  29, Time:   0.0002s\n",
      "      ‚úì  Simulated Annealing            ‚Üí Clique size:  34, Time:   0.3627s\n",
      "      ‚è≠Ô∏è  Bron-Kerbosch                  ‚Üí SKIPPED\n",
      "      ‚úì  Tomita                         ‚Üí Clique size:  34, Time:  66.2457s\n",
      "      ‚úì  Degeneracy BK                  ‚Üí Clique size:  34, Time:  46.7297s\n",
      "      ‚úì  Ostergard                      ‚Üí Clique size:  34, Time:  10.0284s\n",
      "      ‚úì  BBMC                           ‚Üí Clique size:  34, Time:   1.8832s\n",
      "      ‚è≠Ô∏è  CPU Optimized                  ‚Üí SKIPPED\n",
      "      ‚úì  MaxCliqueDyn                   ‚Üí Clique size:  34, Time:   2.5850s\n",
      "   \n",
      "   ‚úÖ Success! 8 algorithms completed\n",
      "   üéØ Best clique: 34 (Simulated_Annealing)\n",
      "   ‚ö° Fastest time: 0.0000s (Greedy)\n",
      "\n",
      "[2/22] Running: C250.9.txt\n",
      "   Category: benchmark\n",
      "   Path: datasets/benchmark/C250.9.txt\n",
      "   üîÑ Starting benchmark execution...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"üöÄ Starting comprehensive benchmark suite\")\n",
    "print(f\"   Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"={'='*100}\\n\")\n",
    "\n",
    "all_results = []\n",
    "failed_datasets = []\n",
    "\n",
    "for idx, (category, name, path) in enumerate(all_datasets, 1):\n",
    "    print(f\"\\n[{idx}/{len(all_datasets)}] Running: {name}\")\n",
    "    print(f\"   Category: {category.split('/')[-1]}\")\n",
    "    print(f\"   Path: {path}\")\n",
    "    \n",
    "    try:\n",
    "        # Run benchmark\n",
    "        print(f\"   üîÑ Starting benchmark execution...\")\n",
    "        sys.stdout.flush()\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Use run() to capture all output at once (avoid streaming duplicates)\n",
    "        result = subprocess.run(\n",
    "            [BENCHMARK_EXECUTABLE, path],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=600  # 10 minute timeout\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        total_time = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            print(f\"   ‚ùå Failed with return code {result.returncode}\")\n",
    "            print(f\"   Error: {result.stderr[:200]}\")\n",
    "            failed_datasets.append(name)\n",
    "            continue\n",
    "        \n",
    "        # Parse output\n",
    "        lines = result.stdout.strip().split('\\n')\n",
    "        \n",
    "        # Extract graph statistics\n",
    "        vertices = edges = density = None\n",
    "        for line in lines:\n",
    "            if 'Vertices:' in line:\n",
    "                try:\n",
    "                    vertices = int(line.split(':')[1].strip().split()[0])\n",
    "                except:\n",
    "                    pass\n",
    "            elif 'Edges:' in line:\n",
    "                try:\n",
    "                    edges = int(line.split(':')[1].strip().split()[0])\n",
    "                except:\n",
    "                    pass\n",
    "            elif 'Density:' in line:\n",
    "                try:\n",
    "                    density = float(line.split(':')[1].strip().split()[0])\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        if vertices is None or edges is None or density is None:\n",
    "            print(f\"   ‚ö†Ô∏è Could not parse graph statistics\")\n",
    "            failed_datasets.append(name)\n",
    "            continue\n",
    "        \n",
    "        print(f\"   üìä Graph: {vertices:,} vertices, {edges:,} edges, density={density:.4f}\")\n",
    "        print(f\"   ‚è±Ô∏è  Total benchmark time: {total_time:.2f}s\")\n",
    "        print(f\"   \")\n",
    "        print(f\"   Algorithm Results:\")\n",
    "        \n",
    "        # Parse algorithm results from RESULTS SUMMARY table\n",
    "        # Format: Algorithm (30 chars), Clique Size (12 chars), Time (15 chars), Memory (15 chars)\n",
    "        dataset_results = []\n",
    "        in_results = False\n",
    "        algo_count = 0\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            # Start parsing after \"RESULTS SUMMARY:\" and the separator line\n",
    "            if 'RESULTS SUMMARY:' in line:\n",
    "                in_results = False  # Will become true after next separator\n",
    "                continue\n",
    "            \n",
    "            # Found the header separator, next lines are data\n",
    "            if in_results == False and line.startswith('---') and i > 0:\n",
    "                # Check if previous line has \"Algorithm\" header\n",
    "                if i > 0 and 'Algorithm' in lines[i-1]:\n",
    "                    in_results = True\n",
    "                    continue\n",
    "            \n",
    "            # Stop at the next separator\n",
    "            if in_results and line.startswith('---'):\n",
    "                break\n",
    "            \n",
    "            if in_results and line.strip():\n",
    "                # Parse: Algorithm(30), Clique Size(12), Time(15), Memory(15)\n",
    "                if len(line) >= 42:\n",
    "                    algo_name = line[:30].strip()\n",
    "                    clique_str = line[30:42].strip()\n",
    "                    time_str = line[42:57].strip()\n",
    "                    \n",
    "                    # Skip FAILED/SKIPPED entries\n",
    "                    if clique_str in ['FAILED', 'SKIPPED', 'N/A']:\n",
    "                        print(f\"      ‚è≠Ô∏è  {algo_name:30s} ‚Üí SKIPPED\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Replace spaces with underscores in algorithm names\n",
    "                    algo_name_clean = algo_name.replace(' ', '_')\n",
    "                    \n",
    "                    try:\n",
    "                        clique_size = int(clique_str)\n",
    "                        time_taken = float(time_str)\n",
    "                        \n",
    "                        dataset_results.append({\n",
    "                            'Dataset': name,\n",
    "                            'Category': category.split('/')[-1],\n",
    "                            'Vertices': vertices,\n",
    "                            'Edges': edges,\n",
    "                            'Density': density,\n",
    "                            'Algorithm': algo_name_clean,\n",
    "                            'CliqueSize': clique_size,\n",
    "                            'Time(s)': time_taken,\n",
    "                            'Success': True\n",
    "                        })\n",
    "                        \n",
    "                        algo_count += 1\n",
    "                        print(f\"      ‚úì  {algo_name:30s} ‚Üí Clique size: {clique_size:3d}, Time: {time_taken:8.4f}s\")\n",
    "                        \n",
    "                    except (ValueError, IndexError) as e:\n",
    "                        # If parsing fails, skip this line\n",
    "                        continue\n",
    "        \n",
    "        if dataset_results:\n",
    "            df_dataset = pd.DataFrame(dataset_results)\n",
    "            all_results.append(df_dataset)\n",
    "            \n",
    "            print(f\"   \")\n",
    "            print(f\"   ‚úÖ Success! {algo_count} algorithms completed\")\n",
    "            \n",
    "            # Show best result\n",
    "            best = df_dataset.loc[df_dataset['CliqueSize'].idxmax()]\n",
    "            print(f\"   üéØ Best clique: {best['CliqueSize']} ({best['Algorithm']})\")\n",
    "            print(f\"   ‚ö° Fastest time: {df_dataset['Time(s)'].min():.4f}s ({df_dataset.loc[df_dataset['Time(s)'].idxmin()]['Algorithm']})\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è No results parsed\")\n",
    "            failed_datasets.append(name)\n",
    "        \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"   ‚è±Ô∏è Timeout (>10 minutes)\")\n",
    "        failed_datasets.append(name)\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        failed_datasets.append(name)\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"‚úÖ Benchmark suite completed!\")\n",
    "print(f\"   Successful: {len(all_results)}/{len(all_datasets)}\")\n",
    "if failed_datasets:\n",
    "    print(f\"   Failed: {len(failed_datasets)} datasets\")\n",
    "    for name in failed_datasets:\n",
    "        print(f\"      ‚Ä¢ {name}\")\n",
    "print(f\"={'='*100}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f2ab0",
   "metadata": {},
   "source": [
    "## 6. Combine and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b014ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results into single dataframe\n",
    "if len(all_results) > 0:\n",
    "    df_all = pd.concat(all_results, ignore_index=True)\n",
    "    \n",
    "    # Save comprehensive CSV\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    csv_path = os.path.join(RESULTS_FOLDER, f\"benchmark_all_{timestamp}.csv\")\n",
    "    df_all.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"üíæ Saved comprehensive results to: {csv_path}\")\n",
    "    print(f\"   Total rows: {len(df_all)}\")\n",
    "    print(f\"   Columns: {', '.join(df_all.columns)}\")\n",
    "    \n",
    "    # Display sample\n",
    "    print(f\"\\nüìã Sample results (first 10 rows):\\n\")\n",
    "    display(df_all.head(10))\n",
    "else:\n",
    "    print(\"‚ùå No results to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c18e3d",
   "metadata": {},
   "source": [
    "## 7. Analysis: Performance by Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ffbcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_results) == 0:\n",
    "    print(\"No results to analyze\")\n",
    "else:\n",
    "    # Filter successful runs only\n",
    "    df_success = df_all[df_all['Success'] == True].copy()\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"üìä ALGORITHM PERFORMANCE SUMMARY\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    # Group by algorithm\n",
    "    agg_dict = {\n",
    "        'CliqueSize': ['mean', 'min', 'max'],\n",
    "        'Time(s)': ['mean', 'median', 'min', 'max'],\n",
    "        'Dataset': 'count'\n",
    "    }\n",
    "    \n",
    "    algo_stats = df_success.groupby('Algorithm').agg(agg_dict).round(4)\n",
    "    \n",
    "    algo_stats.columns = ['_'.join(col).strip() for col in algo_stats.columns.values]\n",
    "    algo_stats = algo_stats.rename(columns={'Dataset_count': 'Runs'})\n",
    "    \n",
    "    # Sort by average time\n",
    "    algo_stats = algo_stats.sort_values('Time(s)_mean')\n",
    "    \n",
    "    display(algo_stats)\n",
    "    \n",
    "    # Save algorithm summary\n",
    "    algo_csv = os.path.join(RESULTS_FOLDER, f\"algorithm_summary_{timestamp}.csv\")\n",
    "    algo_stats.to_csv(algo_csv)\n",
    "    print(f\"\\nüíæ Algorithm summary saved to: {algo_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7bbecb",
   "metadata": {},
   "source": [
    "## 8. Analysis: Performance by Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2389a3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_results) > 0:\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"üìä DATASET ANALYSIS\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    # For each dataset, show best algorithm\n",
    "    for dataset_name in df_all['Dataset'].unique():\n",
    "        df_dataset = df_all[df_all['Dataset'] == dataset_name]\n",
    "        df_dataset_success = df_dataset[df_dataset['Success'] == True]\n",
    "        \n",
    "        if len(df_dataset_success) == 0:\n",
    "            print(f\"‚ö†Ô∏è {dataset_name}: No successful runs\")\n",
    "            continue\n",
    "        \n",
    "        # Graph statistics (same across all algorithms)\n",
    "        stats = df_dataset.iloc[0]\n",
    "        print(f\"\\nüìÅ {dataset_name}\")\n",
    "        print(f\"   Category: {stats['Category']}\")\n",
    "        print(f\"   Graph: {stats['Vertices']:,} vertices, {stats['Edges']:,} edges\")\n",
    "        print(f\"   Density: {stats['Density']:.4f}\")\n",
    "        \n",
    "        # Best clique size\n",
    "        max_clique = df_dataset_success['CliqueSize'].max()\n",
    "        best_algos = df_dataset_success[df_dataset_success['CliqueSize'] == max_clique]\n",
    "        print(f\"   üéØ Best clique size: {max_clique}\")\n",
    "        print(f\"      Found by: {', '.join(best_algos['Algorithm'].values)}\")\n",
    "        \n",
    "        # Fastest algorithm\n",
    "        fastest = df_dataset_success.loc[df_dataset_success['Time(s)'].idxmin()]\n",
    "        print(f\"   ‚ö° Fastest: {fastest['Algorithm']} ({fastest['Time(s)']:.4f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b065037e",
   "metadata": {},
   "source": [
    "## 9. Visualizations: Runtime Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1054d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_results) > 0:\n",
    "    # Runtime comparison by algorithm\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # Box plot of runtimes\n",
    "    df_success_plot = df_success[df_success['Time(s)'] > 0]  # Filter out zero times\n",
    "    \n",
    "    algorithms = df_success_plot['Algorithm'].unique()\n",
    "    data_to_plot = [df_success_plot[df_success_plot['Algorithm'] == algo]['Time(s)'].values \n",
    "                    for algo in algorithms]\n",
    "    \n",
    "    bp = ax.boxplot(data_to_plot, labels=algorithms, patch_artist=True)\n",
    "    \n",
    "    # Color boxes\n",
    "    colors = plt.cm.Set3(range(len(algorithms)))\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    ax.set_ylabel('Runtime (seconds)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Algorithm', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Algorithm Runtime Distribution (All Datasets)', fontsize=14, fontweight='bold')\n",
    "    ax.set_yscale('log')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig_path = os.path.join(RESULTS_FOLDER, f\"runtime_boxplot_{timestamp}.png\")\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üíæ Saved: {fig_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816780e0",
   "metadata": {},
   "source": [
    "## 10. Visualizations: Clique Size Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32e1edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_results) > 0:\n",
    "    # Average clique size by algorithm\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    clique_avg = df_success.groupby('Algorithm')['CliqueSize'].mean().sort_values(ascending=False)\n",
    "    \n",
    "    colors = ['#2ecc71' if val == clique_avg.max() else '#3498db' for val in clique_avg.values]\n",
    "    \n",
    "    ax.bar(range(len(clique_avg)), clique_avg.values, color=colors)\n",
    "    ax.set_xticks(range(len(clique_avg)))\n",
    "    ax.set_xticklabels(clique_avg.index, rotation=45, ha='right')\n",
    "    ax.set_ylabel('Average Clique Size', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Average Maximum Clique Size by Algorithm', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig_path = os.path.join(RESULTS_FOLDER, f\"clique_size_avg_{timestamp}.png\")\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üíæ Saved: {fig_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030cad8d",
   "metadata": {},
   "source": [
    "## 11. Visualizations: Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01b1d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_results) > 0:\n",
    "    print(\"Memory usage data not available in this benchmark format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07da7f92",
   "metadata": {},
   "source": [
    "## 12. Heatmap: Algorithm Performance Across Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b81bfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_results) > 0 and len(df_success['Dataset'].unique()) > 1:\n",
    "    # Create pivot table for heatmap\n",
    "    pivot_time = df_success.pivot_table(\n",
    "        index='Algorithm', \n",
    "        columns='Dataset', \n",
    "        values='Time(s)',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Plot heatmap\n",
    "    fig, ax = plt.subplots(figsize=(18, 10))\n",
    "    sns.heatmap(pivot_time, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax, \n",
    "                cbar_kws={'label': 'Runtime (seconds)'})\n",
    "    ax.set_title('Algorithm Runtime Heatmap Across Datasets', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel('Dataset', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Algorithm', fontsize=12, fontweight='bold')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig_path = os.path.join(RESULTS_FOLDER, f\"heatmap_runtime_{timestamp}.png\")\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üíæ Saved: {fig_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8609e92",
   "metadata": {},
   "source": [
    "## 13. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9776286",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_results) > 0:\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"‚úÖ COMPREHENSIVE BENCHMARK COMPLETE\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    print(f\"üìä Statistics:\")\n",
    "    print(f\"   Total datasets tested: {len(df_all['Dataset'].unique())}\")\n",
    "    print(f\"   Total algorithm runs: {len(df_all)}\")\n",
    "    print(f\"   Successful runs: {len(df_success)} ({len(df_success)/len(df_all)*100:.1f}%)\")\n",
    "    print(f\"   Failed runs: {len(df_all) - len(df_success)}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ Overall Best Performers:\")\n",
    "    \n",
    "    # Fastest overall\n",
    "    fastest_overall = df_success.groupby('Algorithm')['Time(s)'].mean().idxmin()\n",
    "    fastest_time = df_success.groupby('Algorithm')['Time(s)'].mean().min()\n",
    "    print(f\"   ‚ö° Fastest (avg): {fastest_overall} ({fastest_time:.4f}s)\")\n",
    "    \n",
    "    # Best clique finder\n",
    "    best_clique = df_success.groupby('Algorithm')['CliqueSize'].mean().idxmax()\n",
    "    best_clique_size = df_success.groupby('Algorithm')['CliqueSize'].mean().max()\n",
    "    print(f\"   üéØ Best cliques (avg): {best_clique} ({best_clique_size:.2f})\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Results saved to: {RESULTS_FOLDER}/\")\n",
    "    print(f\"   ‚Ä¢ Comprehensive CSV: benchmark_all_{timestamp}.csv\")\n",
    "    print(f\"   ‚Ä¢ Algorithm summary: algorithm_summary_{timestamp}.csv\")\n",
    "    print(f\"   ‚Ä¢ Visualizations: *.png\")\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No results generated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
